**<p align ="center">Collaborative Museum Heist with Reinforcement Learning</p>**
===========================================================
**<p align ="center">Eleni Evripidou, Andreas Aristidou, Panayiotis Charalambous</p>**

<p align ="center">36th International Conference on Computer Animation & Social Agents 2023 (CASA 2023)</p>

**<p align = "center">[DOI](https://doi.org/10.1002/cav.2158) | [Video](https://www.youtube.com/watch?v=-yjui8z7JCw&ab_channel=EleniEvripidou)</p>**

**Abstract:**
<p align = "justify">Non-Playable Characters (NPCs) play a crucial role in enhancing immersion in video games. However, traditional NPC behaviors are often hard-coded using methods such as Finite State Machines, Decision and Behavior trees. This has a few limitations; namely, it is quite difficult to implement complex cooperative behaviors and secondly this makes it easy for human players to identify and exploit patterns in behavior. To overcome these challenges, Reinforcement Learning (RL) can be used to generate dynamic and real-time NPC responses to human player actions. In this paper, we report on first results of applying RL techniques to a Non-Zero Sum, adversarial asymmetric game, using a multi-agent team. The game environment simulates a museum heist, where the objective of the successfully trained team of robbers with different skills (Locksmith, Technician) is to steal valuable items from the museum without being detected by the scripted security guards and cameras. Both agents were trained concurrently with separate policies and received both individual and group reward signals. Through this training process, the agents learned to cooperate effectively and use their skills to maximize both individual and team benefits. These results demonstrate the feasibility of realizing the full game where both robbers and security guards are trained at the same time to achieve their adversarial goals.</p>
